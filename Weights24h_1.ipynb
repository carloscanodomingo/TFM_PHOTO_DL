{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b93f340-0ac0-4007-9652-a38c9bb3a87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, clone_model\n",
    "from keras.layers import Dense  \n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "import os\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import math\n",
    "\n",
    "import datetime as dt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler # for scaling data into [0, 1]  \n",
    "\n",
    "\n",
    "import data_base\n",
    "import openpyxl\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbMetricsLogger, WandbModelCheckpoint\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e689d55-8a2f-490b-bb85-d86d8e9340c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3b4258c-e22c-4c9e-bfcd-fd1203f9745b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\marti/.netrc\n"
     ]
    }
   ],
   "source": [
    "!wandb login d88f8e394954db0905efd5e45c2e5108f96a714a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43c2b491-ba13-4c20-9e74-027bcdffa3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_df = pd.read_csv('real_df_IA.csv')\n",
    "real_df['date_time'] = pd.to_datetime(real_df['date_time'])\n",
    "real_df.set_index('date_time', inplace=True)\n",
    "\n",
    "\n",
    "start = '2021-06-26'\n",
    "end = '2023-05-15'\n",
    "weather_data = real_df.loc[start:end]\n",
    "weather_data = weather_data.drop('GHI', axis=1)\n",
    "weather_data = weather_data.drop('GTI', axis=1)\n",
    "weather_data.loc[weather_data['PAC'] > 38000, 'PAC'] = 38000\n",
    "\n",
    "weather_data = weather_data.resample('H').mean()\n",
    "weather_data = weather_data.fillna(0)\n",
    "df = weather_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1d01c5-c493-4114-9ecd-6b96aa38710a",
   "metadata": {},
   "source": [
    "# VAMOS A CREAR LA FUNCIÓN PARA WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3c02104-4958-4429-b78d-8d49725a7780",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenar_y_evaluar(config=None):\n",
    "    wandb.init(config=config)\n",
    "    config = wandb.config\n",
    "        \n",
    "    ventana_temporal = config.ventana_temporal\n",
    "    caracteristicas_lstm = config.caracteristicas_lstm\n",
    "    ratio_dropout = config.ratio_dropout\n",
    "    tamano_batch = 2048\n",
    "    learning_rate_in = 0.01\n",
    "    numero_de_epoch = 250\n",
    "\n",
    "\n",
    "    \n",
    "#     SE SEPARAN LOS DATOS\n",
    "    PorcentajeEntradas = 80.55\n",
    "    # Número de filas.\n",
    "    nFilas = df.shape[0] \n",
    "    # Cantidad de datos de entrenamiento\n",
    "    noEntrenamiento = int((PorcentajeEntradas / 100) * nFilas) \n",
    "    # Tomamos los datos de entrenemiento\n",
    "    datos_entrenamiento = df[:noEntrenamiento]\n",
    "    # Tomamos los datos de test\n",
    "    datos_test = df[(noEntrenamiento + 1):]\n",
    "    \n",
    "#     SELECCIONAMOS LOS PREDICTORES\n",
    "    #PAC, GHI, TA, WS.\n",
    "\n",
    "    PredictoresSelecionados = ['PAC','GHI_PYR','TA']\n",
    "        \n",
    "\n",
    "\n",
    "    # Solo tenemos un predictor \n",
    "    NoPredictoresSelecionados = len(PredictoresSelecionados)\n",
    "    # Tomamos los indices\n",
    "    posPV = PredictoresSelecionados.index(PredictoresSelecionados[0]) \n",
    "    # Tomamos los valores de los predictores seleccionados para el conjunto de Entrenamiento\n",
    "    procesados_entrenamiento = datos_entrenamiento[PredictoresSelecionados].values \n",
    "    # Tomamos los valores de los predictores seleccionados para el conjunto de test\n",
    "    procesados_test = datos_test[PredictoresSelecionados].values\n",
    "    \n",
    "#   AJUSTAMOS LOS VALORES A 0-1\n",
    "    scaler = MinMaxScaler(feature_range = (0, 1)) \n",
    "    scaler_output = MinMaxScaler(feature_range = (0, 1)) \n",
    "    escalados_entrenamiento = scaler.fit_transform(procesados_entrenamiento) # scales the values into [0, 1]\n",
    "    scaler_output.fit(procesados_entrenamiento[:,0].reshape(-1,1)) # scales the values into [0, 1]\n",
    "#   SE AJUSTAN LOS DATOS DE T = N + 1\n",
    "    features_set = []\n",
    "    labels = []\n",
    "    max_distance = 24\n",
    "     \n",
    "    for i in range(ventana_temporal, len(escalados_entrenamiento)):  \n",
    "        features_set.append(escalados_entrenamiento[i - ventana_temporal:i])\n",
    "        labels.append(escalados_entrenamiento[i, posPV])\n",
    "    labels_direct = []\n",
    "    for i in range(ventana_temporal, len(escalados_entrenamiento) - max_distance):  \n",
    "        labels_direct.append(tf.convert_to_tensor((escalados_entrenamiento[i:i+max_distance, posPV]), dtype=tf.float32))\n",
    "    labels_direct = tf.stack(labels_direct)\n",
    "\n",
    "    # Convertimos las listas a arrays\n",
    "    features_set, labels = np.array(features_set), np.array(labels) \n",
    "    features_set = np.reshape(features_set, (features_set.shape[0], features_set.shape[1], NoPredictoresSelecionados))\n",
    "    \n",
    "#     REPETIMOS PARA EL CONJUNTO DE TEST\n",
    "\n",
    "    # take the dates of the test data for future plotting\n",
    "    # fecha_test = procesados_test\n",
    "    # fecha_test = [onlyDate for [onlyDate] in fecha_test]\n",
    "\n",
    "    # take also the previous hoursBack from training in order to predict the first test hours    \n",
    "    total = pd.concat((datos_entrenamiento[PredictoresSelecionados], datos_test[PredictoresSelecionados]), axis=0)\n",
    "\n",
    "    entradas_test = total[len(total) - len(procesados_test) - ventana_temporal:].values          \n",
    "    entradas_test = entradas_test.reshape(-1, NoPredictoresSelecionados) # puts every item between brackets, values are unchanged\n",
    "    entradas_test_transform = scaler.transform(entradas_test) # also scale the test data \n",
    "\n",
    "    # collect the test feature set as previously for training\n",
    "    test_features = []\n",
    "    for i in range(ventana_temporal, len(procesados_test) + ventana_temporal):  \n",
    "        test_features.append(entradas_test_transform[i-ventana_temporal:i])\n",
    "\n",
    "    # prepare test in numpy array and format for LSTM\n",
    "    test_features = np.array(test_features)\n",
    "    test_features = np.reshape(test_features, (test_features.shape[0], test_features.shape[1], NoPredictoresSelecionados))  \n",
    "    \n",
    "    \n",
    "#     NECESITAMOS LA SALIDA HECHA EN 24H\n",
    "\n",
    "    test_features_output = []\n",
    "    for i in range(24, len(procesados_test) + 24):  \n",
    "        test_features_output.append(entradas_test_transform[i-24:i])\n",
    "\n",
    "    # prepare test in numpy array and format for LSTM\n",
    "    test_features_output = np.array(test_features_output)\n",
    "    test_features_output = np.reshape(test_features_output, (test_features_output.shape[0], test_features_output.shape[1], NoPredictoresSelecionados))\n",
    "\n",
    "#     CREAMOS EL MODELO SECUENCIAL EN KERAS\n",
    "    model = Sequential() \n",
    "    \n",
    "    # Arquitectura = 2 Modelos conectados\n",
    "\n",
    "    model.add(LSTM(units= caracteristicas_lstm, return_sequences=True, input_shape=(features_set.shape[1], NoPredictoresSelecionados)))\n",
    "    # numero de unidades = el tamaño de las caracteristicas del estado de la celda (h_t)\n",
    "    # return_sequences = veradero para concatenar capas LSTM\n",
    "    # Para la primera LSTM el numero de entradas se tiene que especificar\n",
    "    # La forma de entrada es 1 ó más registros, el tamaño de la ventana temporal y el número de predictores.\n",
    "\n",
    "    # Se desactivan aleatoriamente un % de los nodos LSTM\n",
    "    model.add(Dropout(ratio_dropout)) \n",
    "\n",
    "    # Segunda capa LSTM\n",
    "    model.add(LSTM(units=caracteristicas_lstm))  \n",
    "    model.add(Dropout(ratio_dropout)) \n",
    "\n",
    "    # Añadimos una capa totalmente conectada\n",
    "    # units = 1 Solo queremos predecir la siguiente hora.\n",
    "    model.add(Dense(units = max_distance, activation = 'sigmoid')) \n",
    "    \n",
    "#     SE SELECCIONA OPTIMIZER Y FUNCIÓND DE PÉRDIDAS\n",
    "\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate_in)\n",
    "    model.compile(optimizer = optimizer, loss = 'mean_absolute_error')\n",
    "\n",
    "#    SE ENTRENA EL MODELO Y SE SACAN DATOS RELEVANTES\n",
    "    callback_es = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10)\n",
    "    history = model.fit(features_set[0:len(labels_direct)], labels_direct,  epochs = numero_de_epoch,callbacks = [callback_es, WandbMetricsLogger()], batch_size = tamano_batch)\n",
    "    predictions = model.predict(test_features)\n",
    "    labels_test = test_features_output[max_distance:,:,0].squeeze()\n",
    "    predictions = predictions[0:len(labels_test)]\n",
    "    predictions_0 = predictions[:,0].reshape(-1,1)\n",
    "    labels_test_0 = labels_test[:,0].reshape(-1,1)\n",
    "    predictions_23 = predictions[:,23].reshape(-1,1)\n",
    "    labels_test_23 = labels_test[:,23].reshape(-1,1)\n",
    "    \n",
    "    RMSE = math.sqrt(mean_squared_error(scaler_output.inverse_transform(predictions),scaler_output.inverse_transform(labels_test) ))\n",
    "    MAE = mean_absolute_error(scaler_output.inverse_transform(predictions),scaler_output.inverse_transform(labels_test) )\n",
    "    RMSE_0 =  math.sqrt(mean_squared_error(scaler_output.inverse_transform(predictions_0),scaler_output.inverse_transform(labels_test_0) ))\n",
    "    MAE_0 = mean_absolute_error(scaler_output.inverse_transform(predictions_0),scaler_output.inverse_transform(labels_test_0) )\n",
    "    RMSE_23 =  math.sqrt(mean_squared_error(scaler_output.inverse_transform(predictions_23),scaler_output.inverse_transform(labels_test_23) ))\n",
    "    MAE_23 = mean_absolute_error(scaler_output.inverse_transform(predictions_23),scaler_output.inverse_transform(labels_test_23) )\n",
    "    \n",
    "    loss = history.history['loss'][-1]  # Obtener las métricas de pérdida del historial\n",
    "    # wandb.log({\"mae\": mae, \"rmse\": rmse, \"rmse_relative\": rmse_relative})\n",
    "    wandb.log({\"MAE\": MAE, \"RMSE\": RMSE, \"MAE_0\": MAE_0,\"RMSE_0\": RMSE_0,\"MAE_23\": MAE_23, \"RMSE_23\": RMSE_23, \"loss\": loss})   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68e6b90f-d908-4f67-b138-8b49e3106e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'grid'\n",
    "    }\n",
    "parameters_dict = {\n",
    "    'count': {\n",
    "        'values': [0,1,2]\n",
    "        },\n",
    "    'ventana_temporal': {\n",
    "        'values': [72]\n",
    "        },\n",
    "    'caracteristicas_lstm': {\n",
    "          'values': [24]\n",
    "        },   \n",
    "    'ratio_dropout': {\n",
    "          'values': [0.35]\n",
    "        },   \n",
    "    }\n",
    "\n",
    "sweep_config['parameters'] = parameters_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b095417-724f-4cb7-a9ee-98525721a0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: s5u3fy86\n",
      "Sweep URL: https://wandb.ai/martinkek/Test_carlos_1/sweeps/s5u3fy86\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"Test_carlos_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c7c445-af3a-409e-ba90-aaf4cefbd408",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: uekbxn1x with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcaracteristicas_lstm: 24\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcount: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tratio_dropout: 0.35\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tventana_temporal: 72\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmartinmartinruiz\u001b[0m (\u001b[33mmartinkek\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\marti\\Desktop\\TFM\\Deep Learning\\Estudio de Predicciones_2\\wandb\\run-20230629_235647-uekbxn1x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/martinkek/Test_carlos_1/runs/uekbxn1x' target=\"_blank\">toasty-sweep-1</a></strong> to <a href='https://wandb.ai/martinkek/Test_carlos_1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/martinkek/Test_carlos_1/sweeps/s5u3fy86' target=\"_blank\">https://wandb.ai/martinkek/Test_carlos_1/sweeps/s5u3fy86</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/martinkek/Test_carlos_1' target=\"_blank\">https://wandb.ai/martinkek/Test_carlos_1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/martinkek/Test_carlos_1/sweeps/s5u3fy86' target=\"_blank\">https://wandb.ai/martinkek/Test_carlos_1/sweeps/s5u3fy86</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/martinkek/Test_carlos_1/runs/uekbxn1x' target=\"_blank\">https://wandb.ai/martinkek/Test_carlos_1/runs/uekbxn1x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "7/7 [==============================] - 4s 238ms/step - loss: 0.3580\n",
      "Epoch 2/250\n",
      "7/7 [==============================] - 2s 229ms/step - loss: 0.2846\n",
      "Epoch 3/250\n",
      "7/7 [==============================] - 2s 229ms/step - loss: 0.2677\n",
      "Epoch 4/250\n",
      "7/7 [==============================] - 2s 229ms/step - loss: 0.2656\n",
      "Epoch 5/250\n",
      "7/7 [==============================] - 2s 237ms/step - loss: 0.2654\n",
      "Epoch 6/250\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.2653\n",
      "Epoch 7/250\n",
      "7/7 [==============================] - 2s 231ms/step - loss: 0.2653\n",
      "Epoch 8/250\n",
      "7/7 [==============================] - 2s 238ms/step - loss: 0.2653\n",
      "Epoch 9/250\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.2653\n",
      "Epoch 10/250\n",
      "4/7 [================>.............] - ETA: 0s - loss: 0.2654"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id, entrenar_y_evaluar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a4ef90-d14f-4607-8683-e8e06ebc3a5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
